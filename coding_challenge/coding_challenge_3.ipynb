{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Problem: Implement the K-nearest neighbors (KNN) algorithm from scratch. Given a\n",
    "dataset and a query point, classify the query based on the majority label of its k-nearest\n",
    "neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label for the query point [5.0, 3.0] is: A\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Function to compute the Euclidean distance between two points\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((np.array(point1) - np.array(point2)) ** 2))\n",
    "\n",
    "# Step 2: Find the k-nearest neighbors\n",
    "def get_k_nearest_neighbors(training_data, query_point, k):\n",
    "    distances = []\n",
    "    \n",
    "    # Calculate the distance between query point and each point in the dataset\n",
    "    for data_point in training_data:\n",
    "        distance = euclidean_distance(query_point, data_point[:-1])  # Exclude the label\n",
    "        distances.append((data_point, distance))\n",
    "    \n",
    "    # Sort by distance and return the k-nearest neighbors\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    neighbors = [distances[i][0] for i in range(k)]\n",
    "    return neighbors\n",
    "\n",
    "# Step 3: Perform majority voting to classify the query point\n",
    "def majority_vote(neighbors):\n",
    "    labels = [neighbor[-1] for neighbor in neighbors]  # Get the labels of neighbors\n",
    "    most_common = Counter(labels).most_common(1)  # Find the label that appears the most\n",
    "    return most_common[0][0]\n",
    "\n",
    "# KNN Classifier\n",
    "def knn_classifier(training_data, query_point, k):\n",
    "    neighbors = get_k_nearest_neighbors(training_data, query_point, k)\n",
    "    return majority_vote(neighbors)\n",
    "\n",
    "# Sample dataset: [features..., label]\n",
    "training_data = [\n",
    "    [2.0, 3.0, 'A'],\n",
    "    [1.0, 1.0, 'B'],\n",
    "    [4.0, 2.0, 'A'],\n",
    "    [6.0, 5.0, 'B'],\n",
    "    [3.0, 3.0, 'A'],\n",
    "    [7.0, 8.0, 'B']\n",
    "]\n",
    "\n",
    "# Example query point\n",
    "query_point = [5.0, 3.0]\n",
    "\n",
    "# Set the number of neighbors (k)\n",
    "k = 3\n",
    "\n",
    "# Perform classification\n",
    "result = knn_classifier(training_data, query_point, k)\n",
    "print(f\"The predicted label for the query point {query_point} is: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Problem: Implement logistic regression in Python. Include training using gradient descent,\n",
    "and calculate class probabilities for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Cost: 0.6917854297965054\n",
      "Iteration 100, Cost: 0.6501295135671598\n",
      "Iteration 200, Cost: 0.6177745587666571\n",
      "Iteration 300, Cost: 0.5884690496913079\n",
      "Iteration 400, Cost: 0.5618923947992228\n",
      "Iteration 500, Cost: 0.5377500037912307\n",
      "Iteration 600, Cost: 0.5157748664608409\n",
      "Iteration 700, Cost: 0.49572746396065936\n",
      "Iteration 800, Cost: 0.4773945770374852\n",
      "Iteration 900, Cost: 0.4605874373337157\n",
      "Trained Weights: [ 0.86987631 -0.35051264 -1.22038895]\n",
      "Predictions: [0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Step 2: Define the cost function (binary cross-entropy) and gradient\n",
    "def compute_cost(X, y, weights):\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, weights))  # Predicted values\n",
    "    cost = (-1/m) * (np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h)))\n",
    "    return cost\n",
    "\n",
    "# Step 3: Perform gradient descent to update weights\n",
    "def gradient_descent(X, y, weights, learning_rate, num_iterations):\n",
    "    m = len(y)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Step 4: Update weights using gradient descent\n",
    "        h = sigmoid(np.dot(X, weights))\n",
    "        gradient = np.dot(X.T, (h - y)) / m\n",
    "        weights -= learning_rate * gradient\n",
    "\n",
    "        # Optionally print cost every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            cost = compute_cost(X, y, weights)\n",
    "            print(f\"Iteration {i}, Cost: {cost}\")\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Step 5: Predict probabilities and class (binary classification)\n",
    "def predict(X, weights):\n",
    "    probabilities = sigmoid(np.dot(X, weights))\n",
    "    return [1 if p >= 0.5 else 0 for p in probabilities]\n",
    "\n",
    "# Step 6: Logistic Regression function\n",
    "def logistic_regression(X, y, learning_rate, num_iterations):\n",
    "    # Initialize weights (one for each feature + bias)\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    \n",
    "    # Train the model using gradient descent\n",
    "    weights = gradient_descent(X, y, weights, learning_rate, num_iterations)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample dataset: [Feature1, Feature2, Bias (x0 = 1)]\n",
    "    X = np.array([\n",
    "        [1, 2, 1],  # Bias term (x0 = 1)\n",
    "        [2, 3, 1],\n",
    "        [3, 4, 1],\n",
    "        [4, 5, 1],\n",
    "        [5, 6, 1]\n",
    "    ])\n",
    "    \n",
    "    # Labels (y)\n",
    "    y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "    # Parameters\n",
    "    learning_rate = 0.01\n",
    "    num_iterations = 1000\n",
    "\n",
    "    # Train logistic regression model\n",
    "    trained_weights = logistic_regression(X, y, learning_rate, num_iterations)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = predict(X, trained_weights)\n",
    "    \n",
    "    print(f\"Trained Weights: {trained_weights}\")\n",
    "    print(f\"Predictions: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Problem: Write a function to perform matrix multiplication without using external libraries\n",
    "like NumPy. Multiply two matrices representing weight matrices and input vectors in a neural\n",
    "network layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of A * B:\n",
      "[58, 64]\n",
      "[139, 154]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define matrix multiplication function\n",
    "def matrix_multiply(A, B):\n",
    "    # Get the dimensions of the matrices\n",
    "    rows_A, cols_A = len(A), len(A[0])\n",
    "    rows_B, cols_B = len(B), len(B[0])\n",
    "    \n",
    "    # Ensure the number of columns in A equals the number of rows in B\n",
    "    if cols_A != rows_B:\n",
    "        raise ValueError(\"Number of columns in A must equal number of rows in B\")\n",
    "    \n",
    "    # Initialize the result matrix with zeros\n",
    "    result = [[0 for _ in range(cols_B)] for _ in range(rows_A)]\n",
    "    \n",
    "    # Perform matrix multiplication\n",
    "    for i in range(rows_A):\n",
    "        for j in range(cols_B):\n",
    "            for k in range(cols_A):\n",
    "                result[i][j] += A[i][k] * B[k][j]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample matrices\n",
    "    A = [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6]\n",
    "    ]\n",
    "    \n",
    "    B = [\n",
    "        [7, 8],\n",
    "        [9, 10],\n",
    "        [11, 12]\n",
    "    ]\n",
    "    \n",
    "    # Perform matrix multiplication\n",
    "    result = matrix_multiply(A, B)\n",
    "    \n",
    "    # Print the result\n",
    "    print(\"Result of A * B:\")\n",
    "    for row in result:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Problem: Write a Python function that deep copies a neural network represented as a\n",
    "nested list of layers, where each layer contains weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Neural Network:\n",
      "[[[1.0, 0.5, 0.1], [0.4, 0.7, 0.3]], [[0.6, 0.8], [0.9, 0.2], [0.3, 0.4]]]\n",
      "\n",
      "Copied Neural Network:\n",
      "[[[0.2, 0.5, 0.1], [0.4, 0.7, 0.3]], [[0.6, 0.8], [0.9, 0.2], [0.3, 0.4]]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Step 1: Define the function to deep copy a neural network\n",
    "def deep_copy_neural_network(network):\n",
    "    # Use Python's built-in deepcopy function to create a deep copy of the network\n",
    "    return copy.deepcopy(network)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample neural network represented as a nested list of layers (each layer has a weight matrix)\n",
    "    neural_network = [\n",
    "        # Layer 1 weights (2x3 matrix)\n",
    "        [\n",
    "            [0.2, 0.5, 0.1],\n",
    "            [0.4, 0.7, 0.3]\n",
    "        ],\n",
    "        # Layer 2 weights (3x2 matrix)\n",
    "        [\n",
    "            [0.6, 0.8],\n",
    "            [0.9, 0.2],\n",
    "            [0.3, 0.4]\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    # Perform a deep copy of the neural network\n",
    "    copied_network = deep_copy_neural_network(neural_network)\n",
    "    \n",
    "    # Modify the original neural network to verify that the copy is independent\n",
    "    neural_network[0][0][0] = 1.0  # Modify an element in the original network\n",
    "    \n",
    "    # Print original and copied networks\n",
    "    print(\"Original Neural Network:\")\n",
    "    print(neural_network)\n",
    "    \n",
    "    print(\"\\nCopied Neural Network:\")\n",
    "    print(copied_network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Problem: Implement a moving average filter for a time series. Given a series of numbers\n",
    "and a window size, return the moving average of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving Average: [20.0, 30.0, 40.0, 50.0, 60.0]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the moving average function\n",
    "def moving_average(time_series, window_size):\n",
    "    result = []\n",
    "    \n",
    "    # Step 2: Calculate moving averages\n",
    "    for i in range(len(time_series) - window_size + 1):\n",
    "        window = time_series[i:i + window_size]  # Get the current window\n",
    "        window_avg = sum(window) / window_size  # Calculate the average of the window\n",
    "        result.append(window_avg)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample time series\n",
    "    time_series = [10, 20, 30, 40, 50, 60, 70]\n",
    "    \n",
    "    # Window size\n",
    "    window_size = 3\n",
    "    \n",
    "    # Compute moving average\n",
    "    result = moving_average(time_series, window_size)\n",
    "    \n",
    "    # Print the result\n",
    "    print(\"Moving Average:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Problem: Write a Python function that applies the ReLU activation function to a list of\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU Output: [0, 3, 0, 7, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the ReLU function\n",
    "def relu(inputs):\n",
    "    # Apply ReLU to each element in the input list\n",
    "    return [max(0, x) for x in inputs]\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample input list\n",
    "    inputs = [-5, 3, -2, 7, -1, 0]\n",
    "    \n",
    "    # Apply ReLU function\n",
    "    result = relu(inputs)\n",
    "    \n",
    "    # Print the result\n",
    "    print(\"ReLU Output:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Problem: Implement gradient descent to train a linear regression model. Minimize the\n",
    "mean squared error by adjusting the model weights iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model: y = 2.02 * x + 2.92\n",
      "Mean Squared Error: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the linear regression model and gradient descent function\n",
    "def gradient_descent(X, y, learning_rate, epochs):\n",
    "    # Initialize weights (w) and bias (b) to 0\n",
    "    w, b = 0.0, 0.0\n",
    "    \n",
    "    n = len(X)  # Number of data points\n",
    "    \n",
    "    # Step 2: Perform gradient descent for the specified number of epochs\n",
    "    for _ in range(epochs):\n",
    "        # Predicted value for each data point\n",
    "        y_pred = w * X + b\n",
    "        \n",
    "        # Compute the gradients\n",
    "        dw = -(2/n) * sum(X * (y - y_pred))  # Gradient w.r.t. weight\n",
    "        db = -(2/n) * sum(y - y_pred)        # Gradient w.r.t. bias\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# Step 3: Define mean squared error function\n",
    "def mean_squared_error(X, y, w, b):\n",
    "    y_pred = w * X + b\n",
    "    return sum((y - y_pred) ** 2) / len(X)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data (X: inputs, y: labels)\n",
    "    X = [1, 2, 3, 4, 5]\n",
    "    y = [5, 7, 9, 11, 13]\n",
    "    \n",
    "    # Learning rate and number of iterations\n",
    "    learning_rate = 0.01\n",
    "    epochs = 1000\n",
    "    \n",
    "    # Convert lists to arrays for mathematical operations\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Train the model using gradient descent\n",
    "    w, b = gradient_descent(X, y, learning_rate, epochs)\n",
    "    \n",
    "    # Output the learned weights and bias\n",
    "    print(f\"Trained model: y = {w:.2f} * x + {b:.2f}\")\n",
    "    \n",
    "    # Calculate and print the mean squared error\n",
    "    mse = mean_squared_error(X, y, w, b)\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Problem: Write a Python function to perform one-hot encoding of categorical labels for a\n",
    "machine learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoded Labels:\n",
      "cat: [0, 1, 0]\n",
      "dog: [1, 0, 0]\n",
      "cat: [0, 1, 0]\n",
      "bird: [0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the one-hot encoding function\n",
    "def one_hot_encode(labels):\n",
    "    unique_labels = list(set(labels))  # Get unique labels\n",
    "    one_hot = []\n",
    "    \n",
    "    # Step 2: Create one-hot encoded vectors\n",
    "    for label in labels:\n",
    "        encoding = [1 if label == unique else 0 for unique in unique_labels]\n",
    "        one_hot.append(encoding)\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample categorical labels\n",
    "    labels = ['cat', 'dog', 'cat', 'bird']\n",
    "    \n",
    "    # Perform one-hot encoding\n",
    "    encoded_labels = one_hot_encode(labels)\n",
    "    \n",
    "    # Print the result\n",
    "    print(\"One-Hot Encoded Labels:\")\n",
    "    for label, encoding in zip(labels, encoded_labels):\n",
    "        print(f\"{label}: {encoding}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Problem: Implement a Python function to calculate the cosine similarity between two\n",
    "vectors, which is often used in text similarity tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9746\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the cosine similarity function\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Step 2: Calculate the dot product\n",
    "    dot_product = sum(v1 * v2 for v1, v2 in zip(vec1, vec2))\n",
    "    \n",
    "    # Step 3: Calculate the magnitudes (lengths) of the vectors\n",
    "    magnitude_vec1 = sum(v1 ** 2 for v1 in vec1) ** 0.5\n",
    "    magnitude_vec2 = sum(v2 ** 2 for v2 in vec2) ** 0.5\n",
    "    \n",
    "    # Step 4: Calculate cosine similarity (avoid division by zero)\n",
    "    if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample vectors\n",
    "    vec1 = [1, 2, 3]\n",
    "    vec2 = [4, 5, 6]\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    result = cosine_similarity(vec1, vec2)\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Cosine Similarity: {result:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Problem: Given a trained neural network in Python, write a function to prune the network\n",
    "by removing neurons that have small or zero weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Neural Network:\n",
      "[[0.1, 0.2, 0]]\n",
      "[[0.5, 0.3, 0]]\n",
      "[[0.2, 0.4]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the pruning function\n",
    "def prune_network(network, threshold):\n",
    "    \"\"\"\n",
    "    Prune neurons in the neural network with weights below the specified threshold.\n",
    "    \n",
    "    :param network: A nested list representing the neural network's layers and weights.\n",
    "    :param threshold: A float representing the weight threshold for pruning.\n",
    "    :return: A pruned version of the neural network.\n",
    "    \"\"\"\n",
    "    pruned_network = []\n",
    "\n",
    "    # Step 2: Iterate through each layer in the network\n",
    "    for layer in network:\n",
    "        pruned_layer = []\n",
    "        for neuron_weights in layer:\n",
    "            # Step 3: Check if the weights are above the threshold\n",
    "            if any(abs(weight) > threshold for weight in neuron_weights):\n",
    "                pruned_layer.append(neuron_weights)  # Keep the neuron\n",
    "        pruned_network.append(pruned_layer)  # Add the pruned layer\n",
    "\n",
    "    return pruned_network\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample neural network: each inner list represents weights of a neuron\n",
    "    network = [\n",
    "        [[0.1, 0.2, 0], [0, 0, 0]],   # Layer 1\n",
    "        [[0, 0, 0], [0.5, 0.3, 0]],   # Layer 2\n",
    "        [[0.2, 0.4], [0, 0.1]]         # Layer 3\n",
    "    ]\n",
    "\n",
    "    # Prune the network with a threshold of 0.1\n",
    "    threshold = 0.1\n",
    "    pruned_network = prune_network(network, threshold)\n",
    "\n",
    "    # Print the pruned network\n",
    "    print(\"Pruned Neural Network:\")\n",
    "    for layer in pruned_network:\n",
    "        print(layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Problem: Implement a Python function to compute the confusion matrix given true and\n",
    "predicted labels for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "dog: {'dog': 2, 'cat': 1}\n",
      "cat: {'dog': 1, 'cat': 2}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the function to compute the confusion matrix\n",
    "def confusion_matrix(true_labels, predicted_labels):\n",
    "    # Step 2: Get unique classes from true labels\n",
    "    classes = list(set(true_labels))\n",
    "    matrix = {cls: {c: 0 for c in classes} for cls in classes}  # Initialize matrix\n",
    "\n",
    "    # Step 3: Populate the confusion matrix\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        matrix[true][pred] += 1\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample true and predicted labels\n",
    "    true_labels = ['cat', 'dog', 'dog', 'cat', 'cat', 'dog']\n",
    "    predicted_labels = ['cat', 'dog', 'cat', 'cat', 'dog', 'dog']\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    for cls, row in cm.items():\n",
    "        print(f\"{cls}: {row}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Problem: Write a Python function that performs mini-batch gradient descent for optimizing\n",
    "the weights of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 weights:\n",
      "[[0.25462057 2.95598253]\n",
      " [1.05513304 3.27163676]]\n",
      "Layer 2 weights:\n",
      "[[-3.11386671]\n",
      " [ 2.48876804]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Step 1: Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Step 2: Define the mini-batch gradient descent function\n",
    "def mini_batch_gradient_descent(X, y, weights, learning_rate=0.01, batch_size=5, epochs=100):\n",
    "    \"\"\"\n",
    "    Perform mini-batch gradient descent to optimize the weights of a neural network.\n",
    "    \n",
    "    :param X: Input data (features).\n",
    "    :param y: True labels (target values).\n",
    "    :param weights: Initial weights of the neural network.\n",
    "    :param learning_rate: The learning rate for weight updates.\n",
    "    :param batch_size: Number of samples per mini-batch.\n",
    "    :param epochs: Number of iterations over the entire dataset.\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Step 3: Shuffle the data for each epoch\n",
    "        combined = list(zip(X, y))\n",
    "        random.shuffle(combined)\n",
    "        X_shuffled, y_shuffled = zip(*combined)\n",
    "        X_shuffled = np.array(X_shuffled)\n",
    "        y_shuffled = np.array(y_shuffled)\n",
    "\n",
    "        # Step 4: Create mini-batches\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_shuffled[i:i + batch_size]\n",
    "            y_batch = y_shuffled[i:i + batch_size]\n",
    "            \n",
    "            # Step 5: Forward pass\n",
    "            layer_input = X_batch\n",
    "            activations = []\n",
    "            for weight in weights:\n",
    "                layer_input = sigmoid(np.dot(layer_input, weight))\n",
    "                activations.append(layer_input)\n",
    "\n",
    "            # Step 6: Compute the loss\n",
    "            loss = activations[-1] - y_batch\n",
    "\n",
    "            # Step 7: Backward pass\n",
    "            gradients = []\n",
    "            for layer in reversed(range(len(weights))):\n",
    "                if layer == len(weights) - 1:  # Output layer\n",
    "                    gradient = loss * sigmoid_derivative(activations[layer])\n",
    "                else:  # Hidden layers\n",
    "                    gradient = np.dot(loss, weights[layer + 1].T) * sigmoid_derivative(activations[layer])\n",
    "                \n",
    "                gradients.append(np.dot(activations[layer - 1].T, gradient) if layer > 0 else np.dot(X_batch.T, gradient))\n",
    "                loss = gradient\n",
    "            \n",
    "            # Step 8: Update weights\n",
    "            for j in range(len(weights)):\n",
    "                weights[j] -= learning_rate * gradients[-(j + 1)]\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample input data (features) and true labels\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y = np.array([[0], [1], [1], [0]])  # XOR problem\n",
    "\n",
    "    # Initialize weights for a simple neural network (2 input neurons, 2 hidden neurons, 1 output neuron)\n",
    "    weights = [\n",
    "        np.random.rand(2, 2),  # Weights between input and hidden layer\n",
    "        np.random.rand(2, 1)   # Weights between hidden and output layer\n",
    "    ]\n",
    "\n",
    "    # Train the neural network using mini-batch gradient descent\n",
    "    optimized_weights = mini_batch_gradient_descent(X, y, weights, learning_rate=0.1, batch_size=2, epochs=10000)\n",
    "\n",
    "    # Print the optimized weights\n",
    "    for i, weight in enumerate(optimized_weights):\n",
    "        print(f\"Layer {i + 1} weights:\\n{weight}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Problem: Implement k-means clustering from scratch. Given a dataset and the number of\n",
    "clusters k , return the cluster assignments for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster assignments for each data point: [0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_centroids(X, k):\n",
    "    \"\"\"Randomly select k data points as initial centroids.\"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    random_indices = np.random.choice(X.shape[0], k, replace=False)\n",
    "    return X[random_indices]\n",
    "\n",
    "def assign_clusters(X, centroids):\n",
    "    \"\"\"Assign each data point to the nearest centroid.\"\"\"\n",
    "    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "def update_centroids(X, labels, k):\n",
    "    \"\"\"Update centroids by calculating the mean of the assigned data points.\"\"\"\n",
    "    new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n",
    "    return new_centroids\n",
    "\n",
    "def k_means(X, k, max_iters=100, tol=1e-4):\n",
    "    \"\"\"Perform K-means clustering.\n",
    "    \n",
    "    :param X: Input data points (shape: n_samples x n_features).\n",
    "    :param k: Number of clusters.\n",
    "    :param max_iters: Maximum number of iterations.\n",
    "    :param tol: Tolerance to check for convergence.\n",
    "    :return: Cluster assignments for each data point.\n",
    "    \"\"\"\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # Step 1: Assign clusters\n",
    "        labels = assign_clusters(X, centroids)\n",
    "        \n",
    "        # Step 2: Update centroids\n",
    "        new_centroids = update_centroids(X, labels, k)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) < tol):\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data: 2D points\n",
    "    X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "                  [10, 2], [10, 4], [10, 0]])\n",
    "\n",
    "    k = 2  # Number of clusters\n",
    "    labels = k_means(X, k)\n",
    "\n",
    "    # Print the cluster assignments\n",
    "    print(\"Cluster assignments for each data point:\", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Problem: Implement a Python function to calculate the softmax of a list of numbers, which\n",
    "is used in multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.65900114 0.24243297 0.09856589]\n",
      "Sum of probabilities: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Compute the softmax of a list of numbers.\n",
    "    \n",
    "    :param logits: A list or NumPy array of raw scores (logits).\n",
    "    :return: A NumPy array of probabilities corresponding to each input score.\n",
    "    \"\"\"\n",
    "    # Subtract the max for numerical stability\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample logits (raw scores)\n",
    "    logits = [2.0, 1.0, 0.1]\n",
    "    \n",
    "    probabilities = softmax(logits)\n",
    "    print(\"Probabilities:\", probabilities)\n",
    "    print(\"Sum of probabilities:\", np.sum(probabilities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Problem: Write a Python function to compute the TF-IDF (Term Frequency-Inverse\n",
    "Document Frequency) for a list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 TF-IDF scores: {'the': -0.11507282898071236, 'cat': 0.0, 'in': 0.0, 'hat': 0.08109302162163289}\n",
      "Document 2 TF-IDF scores: {'the': -0.11507282898071236, 'dog': 0.0, 'in': 0.0, 'fog': 0.08109302162163289}\n",
      "Document 3 TF-IDF scores: {'the': -0.11507282898071236, 'cat': 0.0, 'and': 0.08109302162163289, 'dog': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def compute_tf(doc):\n",
    "    \"\"\"Compute the term frequency (TF) for a single document.\"\"\"\n",
    "    tf_count = Counter(doc)\n",
    "    tf = {word: count / len(doc) for word, count in tf_count.items()}\n",
    "    return tf\n",
    "\n",
    "def compute_idf(documents):\n",
    "    \"\"\"Compute the inverse document frequency (IDF) for the list of documents.\"\"\"\n",
    "    N = len(documents)\n",
    "    idf = {}\n",
    "    all_words = set(word for doc in documents for word in doc)\n",
    "\n",
    "    for word in all_words:\n",
    "        count = sum(1 for doc in documents if word in doc)\n",
    "        idf[word] = math.log(N / (1 + count))  # Adding 1 to avoid division by zero\n",
    "\n",
    "    return idf\n",
    "\n",
    "def compute_tfidf(documents):\n",
    "    \"\"\"Compute the TF-IDF for a list of documents.\"\"\"\n",
    "    tfidf = []\n",
    "    idf = compute_idf(documents)\n",
    "\n",
    "    for doc in documents:\n",
    "        tf = compute_tf(doc)\n",
    "        tfidf_doc = {word: tf[word] * idf[word] for word in doc}\n",
    "        tfidf.append(tfidf_doc)\n",
    "\n",
    "    return tfidf\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    documents = [\n",
    "        [\"the\", \"cat\", \"in\", \"the\", \"hat\"],\n",
    "        [\"the\", \"dog\", \"in\", \"the\", \"fog\"],\n",
    "        [\"the\", \"cat\", \"and\", \"the\", \"dog\"]\n",
    "    ]\n",
    "    \n",
    "    tfidf_scores = compute_tfidf(documents)\n",
    "    for i, doc in enumerate(tfidf_scores):\n",
    "        print(f\"Document {i+1} TF-IDF scores: {doc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Problem: Implement an algorithm to find the principal components of a dataset using\n",
    "singular value decomposition (SVD), which is a core part of PCA (Principal Component\n",
    "Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected data:\n",
      " [[-0.82797019]\n",
      " [ 1.77758033]\n",
      " [-0.99219749]\n",
      " [-0.27421042]\n",
      " [-1.67580142]\n",
      " [-0.9129491 ]\n",
      " [ 0.09910944]\n",
      " [ 1.14457216]\n",
      " [ 0.43804614]\n",
      " [ 1.22382056]]\n",
      "Principal components:\n",
      " [[-0.6778734 ]\n",
      " [-0.73517866]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pca_svd(X, num_components):\n",
    "    \"\"\"\n",
    "    Perform PCA using Singular Value Decomposition (SVD).\n",
    "    \n",
    "    :param X: Input data matrix (shape: n_samples x n_features).\n",
    "    :param num_components: Number of principal components to return.\n",
    "    :return: Projected data onto the principal components.\n",
    "    \"\"\"\n",
    "    # Step 1: Center the data (subtract the mean)\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "    # Step 2: Perform Singular Value Decomposition\n",
    "    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "\n",
    "    # Step 3: Select the first num_components principal components\n",
    "    W = Vt[:num_components].T  # Get the first `num_components` rows and transpose\n",
    "    \n",
    "    # Step 4: Project the data onto the principal components\n",
    "    X_pca = X_centered.dot(W)\n",
    "\n",
    "    return X_pca, W\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data: 5 samples with 3 features\n",
    "    X = np.array([[2.5, 2.4],\n",
    "                  [0.5, 0.7],\n",
    "                  [2.2, 2.9],\n",
    "                  [1.9, 2.2],\n",
    "                  [3.1, 3.0],\n",
    "                  [2.3, 2.7],\n",
    "                  [2, 1.6],\n",
    "                  [1, 1.1],\n",
    "                  [1.5, 1.6],\n",
    "                  [1.1, 0.9]])\n",
    "\n",
    "    num_components = 1  # Number of principal components\n",
    "    X_pca, components = pca_svd(X, num_components)\n",
    "\n",
    "    print(\"Projected data:\\n\", X_pca)\n",
    "    print(\"Principal components:\\n\", components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Problem: Write a Python function to calculate the AUC-ROC score for a binary\n",
    "classification problem, given the true labels and predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC score: 0.92\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def calculate_auc_roc(true_labels, predicted_probs):\n",
    "    \"\"\"\n",
    "    Calculate the AUC-ROC score and plot the ROC curve.\n",
    "    \n",
    "    :param true_labels: Array-like of true binary labels (0 or 1).\n",
    "    :param predicted_probs: Array-like of predicted probabilities for the positive class.\n",
    "    :return: AUC score.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute the ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, predicted_probs)\n",
    "    \n",
    "    # Step 2: Calculate the AUC score\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    \n",
    "    # Step 3: Plot the ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))\n",
    "    plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')  # Diagonal line\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return auc_score\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample true labels and predicted probabilities\n",
    "    true_labels = np.array([0, 0, 1, 1, 0, 1, 1, 0, 1, 0])\n",
    "    predicted_probs = np.array([0.1, 0.4, 0.35, 0.8, 0.2, 0.95, 0.85, 0.5, 0.6, 0.3])\n",
    "    \n",
    "    auc_score = calculate_auc_roc(true_labels, predicted_probs)\n",
    "    print(\"AUC-ROC score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Problem: Implement a Python function to apply dropout regularization to the neurons of a\n",
    "given neural network during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input:\n",
      " [[0.2 0.8 0.5 0.1]\n",
      " [0.4 0.3 0.6 0.7]\n",
      " [0.5 0.2 0.1 0.9]\n",
      " [0.6 0.4 0.3 0.2]\n",
      " [0.9 0.5 0.2 0.8]]\n",
      "Input After Dropout:\n",
      " [[0.28571429 0.         0.         0.        ]\n",
      " [0.57142857 0.42857143 0.85714286 0.        ]\n",
      " [0.71428571 0.28571429 0.         0.        ]\n",
      " [0.         0.57142857 0.42857143 0.28571429]\n",
      " [0.         0.71428571 0.         0.        ]]\n",
      "Dropout Mask:\n",
      " [[ True False False False]\n",
      " [ True  True  True False]\n",
      " [ True  True False False]\n",
      " [False  True  True  True]\n",
      " [False  True False False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dropout(X, dropout_rate):\n",
    "    \"\"\"\n",
    "    Apply dropout to the input data.\n",
    "\n",
    "    :param X: Input data (shape: n_samples x n_neurons).\n",
    "    :param dropout_rate: The probability of dropping a neuron (between 0 and 1).\n",
    "    :return: Data after applying dropout and the dropout mask.\n",
    "    \"\"\"\n",
    "    # Generate a dropout mask\n",
    "    mask = np.random.rand(*X.shape) > dropout_rate\n",
    "    # Scale the output by the inverse of (1 - dropout_rate) during training\n",
    "    scaled_output = X * mask / (1 - dropout_rate)\n",
    "    return scaled_output, mask\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample input data (5 samples with 4 neurons)\n",
    "    X = np.array([[0.2, 0.8, 0.5, 0.1],\n",
    "                  [0.4, 0.3, 0.6, 0.7],\n",
    "                  [0.5, 0.2, 0.1, 0.9],\n",
    "                  [0.6, 0.4, 0.3, 0.2],\n",
    "                  [0.9, 0.5, 0.2, 0.8]])\n",
    "\n",
    "    dropout_rate = 0.3  # 30% dropout\n",
    "    X_dropped, mask = dropout(X, dropout_rate)\n",
    "\n",
    "    print(\"Original Input:\\n\", X)\n",
    "    print(\"Input After Dropout:\\n\", X_dropped)\n",
    "    print(\"Dropout Mask:\\n\", mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Problem: Write a Python function to perform feature scaling using standardization (z-score\n",
    "normalization), which transforms features to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input:\n",
      " [[1. 2. 3.]\n",
      " [2. 3. 4.]\n",
      " [3. 4. 5.]\n",
      " [4. 5. 6.]\n",
      " [5. 6. 7.]]\n",
      "Standardized Output:\n",
      " [[-1.41421356 -1.41421356 -1.41421356]\n",
      " [-0.70710678 -0.70710678 -0.70710678]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.70710678  0.70710678  0.70710678]\n",
      " [ 1.41421356  1.41421356  1.41421356]]\n",
      "Mean of Standardized Output:\n",
      " [0. 0. 0.]\n",
      "Std Dev of Standardized Output:\n",
      " [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def standardize(X):\n",
    "    \"\"\"\n",
    "    Standardize the features of the dataset using z-score normalization.\n",
    "\n",
    "    :param X: Input data (shape: n_samples x n_features).\n",
    "    :return: Standardized data with mean 0 and standard deviation 1.\n",
    "    \"\"\"\n",
    "    # Calculate the mean and standard deviation for each feature\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std_dev = np.std(X, axis=0)\n",
    "    \n",
    "    # Standardize the data\n",
    "    standardized_X = (X - mean) / std_dev\n",
    "    \n",
    "    return standardized_X\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample input data (5 samples with 3 features)\n",
    "    X = np.array([[1.0, 2.0, 3.0],\n",
    "                  [2.0, 3.0, 4.0],\n",
    "                  [3.0, 4.0, 5.0],\n",
    "                  [4.0, 5.0, 6.0],\n",
    "                  [5.0, 6.0, 7.0]])\n",
    "\n",
    "    standardized_X = standardize(X)\n",
    "\n",
    "    print(\"Original Input:\\n\", X)\n",
    "    print(\"Standardized Output:\\n\", standardized_X)\n",
    "    print(\"Mean of Standardized Output:\\n\", np.mean(standardized_X, axis=0))\n",
    "    print(\"Std Dev of Standardized Output:\\n\", np.std(standardized_X, axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Problem: Implement a Python function to compute the cross-entropy loss for a multi-class\n",
    "classification problem, given the true labels and predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 0.44594782489471957\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss for multi-class classification.\n",
    "\n",
    "    :param y_true: True labels (one-hot encoded or integer class labels).\n",
    "    :param y_pred: Predicted probabilities for each class (shape: n_samples x n_classes).\n",
    "    :return: Cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Ensure y_pred is a proper probability distribution\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # Clip to avoid log(0)\n",
    "    \n",
    "    # If y_true is in one-hot encoded format, we can compute the loss directly\n",
    "    if y_true.ndim == 1:\n",
    "        # Convert integer class labels to one-hot encoding\n",
    "        n_samples = y_true.shape[0]\n",
    "        n_classes = y_pred.shape[1]\n",
    "        one_hot_y_true = np.zeros((n_samples, n_classes))\n",
    "        one_hot_y_true[np.arange(n_samples), y_true] = 1\n",
    "        y_true = one_hot_y_true\n",
    "\n",
    "    # Compute the cross-entropy loss\n",
    "    loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    return loss\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # True labels (can be integer class labels or one-hot encoded)\n",
    "    y_true = np.array([0, 1, 2, 1])  # Integer class labels for 4 samples\n",
    "\n",
    "    # Predicted probabilities (for 3 classes)\n",
    "    y_pred = np.array([[0.7, 0.2, 0.1],  # Prediction for sample 1\n",
    "                       [0.1, 0.8, 0.1],  # Prediction for sample 2\n",
    "                       [0.2, 0.3, 0.5],  # Prediction for sample 3\n",
    "                       [0.3, 0.6, 0.1]]) # Prediction for sample 4\n",
    "\n",
    "    loss = cross_entropy_loss(y_true, y_pred)\n",
    "\n",
    "    print(\"Cross-Entropy Loss:\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
